# æ–°å»ºä¸€ä¸ªæ–‡ä»¶ä¸“é—¨ç®¡ç†çŸ¥è¯†åº“ã€‚è¿™ä¸ªæ¨¡å—è´Ÿè´£æŠŠæ–‡æœ¬å˜æˆå‘é‡å­˜èµ·æ¥ï¼Œä»¥åŠæŠŠå‘é‡æŸ¥å‡ºæ¥
# knowledge_engine.py
import os
import chromadb
from chromadb.utils import embedding_functions
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pypdf import PdfReader
import pdfplumber

# ===============================
# 1. è®¡ç®—é¡¹ç›®æ ¹ç›®å½•
# knowledge_engine.py æ‰€åœ¨è·¯å¾„ï¼šinvestment_agent_crewai/agent_system/knowledge/knowledge_engine.py
# ===============================
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))

PROJECT_ROOT = os.path.abspath(
    os.path.join(CURRENT_DIR, "../../")
)

# ===============================
# 2. ChromaDB æŒä¹…åŒ–ç›®å½•
# ===============================
CHROMA_DATA_PATH = os.path.join(PROJECT_ROOT, "chroma_db")
os.makedirs(CHROMA_DATA_PATH, exist_ok=True)

# ===============================
# 3. åˆå§‹åŒ– Chroma Client
# ===============================
client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)

# 2. è®¾ç½®å‘é‡æ¨¡å‹ (ä½¿ç”¨å¼€æºå…è´¹çš„ huggingface æ¨¡å‹ï¼Œæ”¯æŒä¸­æ–‡)
# ç¬¬ä¸€æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œçº¦ 500MB
emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="BAAI/bge-m3" # è¿™æ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„æ”¯æŒä¸­è‹±æ–‡çš„ Embedding æ¨¡å‹
)

# 3. è·å–æˆ–åˆ›å»ºé›†åˆ (Collection)
collection = client.get_or_create_collection(
    name="industry_research_db",
    embedding_function=emb_fn
)

class KnowledgeBaseManager:
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,  # æ¯ä¸ªåˆ‡ç‰‡500å­—
            chunk_overlap=50 # åˆ‡ç‰‡ä¹‹é—´é‡å 50å­—ï¼Œé˜²æ­¢è¯­ä¹‰æ–­è£‚
        )
        
    # --- æ ¸å¿ƒåŠŸèƒ½ï¼šè®© Agent å˜èªæ˜çš„â€œåƒä¹¦â€è¿‡ç¨‹ ---
    # åºŸå¼ƒé€šç”¨çš„ read_pdf ç”¨äºâ€œå¯»æ‰¾æ•°æ®â€ã€‚å°† read_pdf æ”¹é€ æˆ get_table_of_contents (è¯»å–ç›®å½•) å·¥å…·ã€‚
    # Agent å…ˆçœ‹ç›®å½•ï¼ŒçŸ¥é“å“ªä¸€ç« è®²è´¢åŠ¡ï¼Œç„¶åå†ç”¨ RAG å»æœé‚£ä¸€ç« çš„ç»†èŠ‚ã€‚
    def ingest_pdf(self, file_path):
        """è¯»å–PDF -> åˆ‡ç‰‡ -> å‘é‡åŒ– -> å­˜å…¥DB"""
        print(f"ğŸ“¥ æ­£åœ¨æ·±åº¦è§£ææ–‡ä»¶ (å«è¡¨æ ¼): {file_path} ...")
        full_text = ""
        
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                # 1. æå–çº¯æ–‡æœ¬
                text = page.extract_text() or ""
                
                # 2. æå–è¡¨æ ¼ (è¿™æ˜¯ pypdf åšä¸åˆ°çš„)
                tables = page.extract_tables()
                table_text = ""
                for table in tables:
                    # å°†è¡¨æ ¼è½¬æ¢ä¸º Markdown æ ¼å¼ï¼Œæ–¹ä¾¿ LLM ç†è§£
                    # ç®€å•å¤„ç†ï¼šè¿‡æ»¤ Noneï¼Œè½¬å­—ç¬¦ä¸²
                    cleaned_table = [[str(cell) if cell else "" for cell in row] for row in table]
                    # è¿™é‡Œå¯ä»¥å†™ä¸€ä¸ªå‡½æ•°æŠŠ list è½¬ markdown table string
                    table_text += f"\n[è¡¨æ ¼æ•°æ®]: {str(cleaned_table)}\n"
                
                full_text += text + "\n" + table_text
        
        # 2. æ–‡æœ¬åˆ‡ç‰‡
        chunks = self.text_splitter.split_text(full_text)
        
        # 3. æ„é€ å…ƒæ•°æ® (Metadata)ï¼Œæ–¹ä¾¿åç»­è¿‡æ»¤
        filename = os.path.basename(file_path)
        ids = [f"{filename}_{i}" for i in range(len(chunks))]
        metadatas = [{"source": filename, "type": "report"} for _ in range(len(chunks))]
        
        # 4. å­˜å…¥ ChromaDB (ä¼šè‡ªåŠ¨è°ƒç”¨ embedding æ¨¡å‹è½¬å‘é‡)
        collection.add(
            documents=chunks,
            ids=ids,
            metadatas=metadatas
        )
        print(f"âœ… å·²å­˜å…¥ {len(chunks)} ä¸ªçŸ¥è¯†ç‰‡æ®µã€‚")

    # --- æ ¸å¿ƒåŠŸèƒ½ï¼šè®© Agent å˜èªæ˜çš„â€œå›å¿†â€è¿‡ç¨‹ ---
   # ä½ ä½¿ç”¨äº† BAAI/bge-m3 è¿›è¡Œå‘é‡æ£€ç´¢ã€‚å‘é‡æ£€ç´¢æ˜¯åŸºäºâ€œè¯­ä¹‰ç›¸ä¼¼åº¦â€çš„ã€‚ é—®é¢˜ï¼š
   # å½“ä½ é—®â€œ2024å¹´è¥æ”¶æ˜¯å¤šå°‘â€æ—¶ï¼Œå‘é‡æ£€ç´¢å¯èƒ½ä¼šæ‰¾å›æ¥â€œ2023å¹´è¥æ”¶â€æˆ–è€…â€œ2024å¹´åˆ©æ¶¦â€ï¼Œå› ä¸ºå®ƒä»¬åœ¨è¯­ä¹‰ä¸Šå¾ˆåƒã€‚
   # é‡‘èåˆ†æéœ€è¦ç²¾ç¡®åŒ¹é…ã€‚æŠ•èµ„äººä¸èƒ½æ¥å—â€œå¤§æ¦‚æ˜¯è¿™ä¸ªæ•°â€ã€‚
   # ä»£ç ä½“ç°ï¼šknowledge_engine.py ä¸­åªæœ‰ collection.queryï¼ˆçº¯å‘é‡æœç´¢ï¼‰ã€‚
   # æ”¹è¿›æ–¹æ¡ˆï¼šæ··åˆæ£€ç´¢ (Hybrid Search) åŒæ—¶è¿›è¡Œå…³é”®è¯æ£€ç´¢ï¼ˆBM25ï¼‰å’Œå‘é‡æ£€ç´¢ï¼Œå¹¶è¿›è¡Œé‡æ’åºï¼ˆRerankï¼‰ã€‚
   # `å“ªæ€•ä¸åšé‚£ä¹ˆå¤æ‚ï¼Œè‡³å°‘è¦åœ¨ Tool é‡Œå¢åŠ å…³é”®è¯è¿‡æ»¤ã€‚
    def query_knowledge(self, query, n_results=5, keyword_filter=None):
        """
        æ ¹æ®é—®é¢˜ï¼Œåœ¨æ•°æ®åº“ä¸­å¯»æ‰¾æœ€ç›¸å…³çš„è¯æ®
        keyword_filter: å¼ºåˆ¶è¦æ±‚ç»“æœä¸­åŒ…å«ç‰¹å®šè¯ï¼ˆå¦‚å¹´ä»½ã€æŒ‡æ ‡åï¼‰
        å¢åŠ å…³é”®è¯è¿‡æ»¤èƒ½åŠ›
        """
        results = collection.query(
            query_texts=[query],
            n_results=n_results * 2 # å¤šå–ä¸€ç‚¹ç”¨æ¥è¿‡æ»¤
        )
        
        docs = results['documents'][0]
        metadatas = results['metadatas'][0]
        
        final_results = []
        for doc, meta in zip(docs, metadatas):
            # ç®€å•çš„ç¡¬è¿‡æ»¤ï¼šå¦‚æœæŒ‡å®šäº†å…³é”®è¯ï¼Œå¿…é¡»åŒ…å«
            if keyword_filter and keyword_filter not in doc:
                continue
            
            # æ‹¼å‡‘å¼•ç”¨æ¥æºï¼Œè§£å†³ä¿¡ä»»é—®é¢˜ï¼ˆç—›ç‚¹äºŒï¼‰
            source_info = f"[æ¥æº: {meta['source']}]" 
            final_results.append(f"{source_info}\n{doc}")
            
        return "\n\n".join(final_results[:n_results])

# å®ä¾‹åŒ–
kb_manager = KnowledgeBaseManager()

