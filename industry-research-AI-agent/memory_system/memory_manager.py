# memory_system/memory_manager.py
"""
å…¨ç»´æŠ•ç ”è®°å¿†ç³»ç»Ÿ - å¢å¼ºç‰ˆ
æ ¸å¿ƒæ”¹è¿›ï¼š
1. æ™ºèƒ½å­¦ä¹ æœºåˆ¶ï¼šä»å†å²ç ”æŠ¥ä¸­å­¦ä¹ æœ€ä½³å®è·µ
2. ç»éªŒç§¯ç´¯ï¼šè®°å½•æˆåŠŸçš„ç ”ç©¶æ¨¡å¼å’Œå¤±è´¥æ¡ˆä¾‹
3. ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼šæ ¹æ®è¡Œä¸šç‰¹å¾æä¾›é’ˆå¯¹æ€§å»ºè®®
4. çŸ¥è¯†å›¾è°±ï¼šæ„å»ºè¡Œä¸šå…³è”çŸ¥è¯†ç½‘ç»œ
"""

import datetime
import json
import hashlib
from typing import Dict, List, Optional, Any, Tuple
from collections import defaultdict

from ingestion.pdf_ingest import PDFIngestor
from memory_system.vector_store.chroma_client import ChromaVectorStore
from rag.retriever import VectorRetriever
from langchain.text_splitter import RecursiveCharacterTextSplitter


class IndustryKnowledgeGraph:
    """
    è¡Œä¸šçŸ¥è¯†å›¾è°±
    è®°å½•è¡Œä¸šé—´çš„å…³è”å…³ç³»ã€äº§ä¸šé“¾ç»“æ„ã€å…³é”®æŒ‡æ ‡ç­‰
    """
    
    def __init__(self):
        self.industry_relations: Dict[str, Dict] = {}  # è¡Œä¸šå…³è”å…³ç³»
        self.supply_chain_templates: Dict[str, Dict] = {}  # äº§ä¸šé“¾æ¨¡æ¿
        self.key_metrics: Dict[str, List[str]] = {}  # è¡Œä¸šå…³é”®æŒ‡æ ‡
        self._init_default_knowledge()
    
    def _init_default_knowledge(self):
        """åˆå§‹åŒ–é»˜è®¤è¡Œä¸šçŸ¥è¯†"""
        # è¡Œä¸šå…³è”å…³ç³»
        self.industry_relations = {
            "äººå·¥æ™ºèƒ½": {
                "upstream": ["åŠå¯¼ä½“", "èŠ¯ç‰‡è®¾è®¡", "æ•°æ®æœåŠ¡", "äº‘è®¡ç®—"],
                "downstream": ["æ™ºèƒ½åˆ¶é€ ", "æ™ºæ…§åŒ»ç–—", "è‡ªåŠ¨é©¾é©¶", "é‡‘èç§‘æŠ€"],
                "related": ["å¤§æ•°æ®", "ç‰©è”ç½‘", "5Gé€šä¿¡"]
            },
            "æ–°èƒ½æºæ±½è½¦": {
                "upstream": ["é”‚ç”µæ± ", "ç”µæœº", "ç”µæ§ç³»ç»Ÿ", "ç¨€åœŸææ–™"],
                "downstream": ["å‡ºè¡ŒæœåŠ¡", "å……ç”µæ¡©", "æ±½è½¦åå¸‚åœº"],
                "related": ["æ™ºèƒ½é©¾é©¶", "è½¦è”ç½‘", "å‚¨èƒ½"]
            },
            "åŠå¯¼ä½“": {
                "upstream": ["ç¡…ç‰‡", "å…‰åˆ»æœº", "EDAå·¥å…·", "ç‰¹ç§æ°”ä½“"],
                "downstream": ["æ¶ˆè´¹ç”µå­", "æ±½è½¦ç”µå­", "å·¥ä¸šæ§åˆ¶"],
                "related": ["äººå·¥æ™ºèƒ½", "5Gé€šä¿¡", "ç‰©è”ç½‘"]
            },
            "ç”Ÿç‰©åŒ»è¯": {
                "upstream": ["åŸæ–™è¯", "åŒ»ç–—å™¨æ¢°", "CRO/CDMO"],
                "downstream": ["åŒ»é™¢", "è¯åº—", "åŒ»ç–—æœåŠ¡"],
                "related": ["åŸºå› æ£€æµ‹", "ç²¾å‡†åŒ»ç–—", "åŒ»ç–—AI"]
            }
        }
        
        # äº§ä¸šé“¾åˆ†ææ¨¡æ¿
        self.supply_chain_templates = {
            "default": {
                "upstream": ["åŸææ–™ä¾›åº”", "æ ¸å¿ƒé›¶éƒ¨ä»¶", "è®¾å¤‡ä¾›åº”å•†"],
                "midstream": ["æ ¸å¿ƒåˆ¶é€ ", "æŠ€æœ¯ç ”å‘", "ç³»ç»Ÿé›†æˆ"],
                "downstream": ["ç»ˆç«¯åº”ç”¨", "æ¸ é“åˆ†é”€", "å”®åæœåŠ¡"]
            },
            "äººå·¥æ™ºèƒ½": {
                "upstream": ["AIèŠ¯ç‰‡", "ç®—åŠ›åŸºç¡€è®¾æ–½", "æ•°æ®æœåŠ¡", "å¼€å‘æ¡†æ¶"],
                "midstream": ["ç®—æ³•ç ”å‘", "æ¨¡å‹è®­ç»ƒ", "å¹³å°æœåŠ¡"],
                "downstream": ["è¡Œä¸šåº”ç”¨", "æ¶ˆè´¹çº§äº§å“", "è§£å†³æ–¹æ¡ˆ"]
            }
        }
        
        # è¡Œä¸šå…³é”®æŒ‡æ ‡
        self.key_metrics = {
            "äººå·¥æ™ºèƒ½": [
                "æ ¸å¿ƒäº§ä¸šè§„æ¨¡", "ä¼ä¸šæ•°é‡", "ä¸“åˆ©ç”³è¯·é‡", "èèµ„è§„æ¨¡",
                "ç®—åŠ›è§„æ¨¡", "äººæ‰æ•°é‡", "åº”ç”¨æ¸—é€ç‡"
            ],
            "æ–°èƒ½æºæ±½è½¦": [
                "äº§é”€é‡", "æ¸—é€ç‡", "ç”µæ± è£…æœºé‡", "å……ç”µæ¡©æ•°é‡",
                "å‡ºå£é‡", "å¸‚åœºé›†ä¸­åº¦"
            ],
            "åŠå¯¼ä½“": [
                "äº§å€¼è§„æ¨¡", "è®¾è®¡ä¼ä¸šæ•°é‡", "åˆ¶é€ äº§èƒ½", "å›½äº§åŒ–ç‡",
                "ç ”å‘æŠ•å…¥", "ä¸“åˆ©æ•°é‡"
            ]
        }
    
    def get_related_industries(self, industry: str) -> Dict[str, List[str]]:
        """è·å–ç›¸å…³è¡Œä¸š"""
        return self.industry_relations.get(industry, {
            "upstream": [],
            "downstream": [],
            "related": []
        })
    
    def get_supply_chain_template(self, industry: str) -> Dict[str, List[str]]:
        """è·å–äº§ä¸šé“¾æ¨¡æ¿"""
        return self.supply_chain_templates.get(
            industry, 
            self.supply_chain_templates["default"]
        )
    
    def get_key_metrics(self, industry: str) -> List[str]:
        """è·å–è¡Œä¸šå…³é”®æŒ‡æ ‡"""
        return self.key_metrics.get(industry, [
            "å¸‚åœºè§„æ¨¡", "å¢é•¿ç‡", "ç«äº‰æ ¼å±€", "æ”¿ç­–æ”¯æŒ", "æŠ€æœ¯è¶‹åŠ¿"
        ])


class ResearchExperience:
    """
    ç ”ç©¶ç»éªŒç®¡ç†å™¨
    è®°å½•å’Œå­¦ä¹ æˆåŠŸçš„ç ”ç©¶æ¨¡å¼
    """
    
    def __init__(self):
        self.successful_patterns: List[Dict] = []  # æˆåŠŸçš„ç ”ç©¶æ¨¡å¼
        self.failed_patterns: List[Dict] = []  # å¤±è´¥æ¡ˆä¾‹
        self.best_practices: Dict[str, List[str]] = {}  # æœ€ä½³å®è·µ
        self.quality_scores: Dict[str, float] = {}  # ç ”æŠ¥è´¨é‡è¯„åˆ†
        self._init_best_practices()
    
    def _init_best_practices(self):
        """åˆå§‹åŒ–æœ€ä½³å®è·µ"""
        self.best_practices = {
            "æ•°æ®å¼•ç”¨": [
                "æ‰€æœ‰æ•°æ®å¿…é¡»æ ‡æ³¨æ¥æºå’Œæ—¶é—´",
                "ä¼˜å…ˆä½¿ç”¨å®˜æ–¹ç»Ÿè®¡æ•°æ®å’Œæƒå¨æœºæ„æŠ¥å‘Š",
                "å¯¹æ¯”å¤šä¸ªæ•°æ®æºè¿›è¡Œäº¤å‰éªŒè¯",
                "æ³¨æ˜æ•°æ®çš„ç»Ÿè®¡å£å¾„å’Œå®šä¹‰"
            ],
            "äº§ä¸šé“¾åˆ†æ": [
                "æ˜ç¡®ä¸Šä¸­ä¸‹æ¸¸çš„åˆ’åˆ†æ ‡å‡†",
                "åˆ†æå„ç¯èŠ‚çš„ä»·å€¼åˆ†é…å’Œè®®ä»·èƒ½åŠ›",
                "è¯†åˆ«äº§ä¸šé“¾çš„å…³é”®å¡è„–å­ç¯èŠ‚",
                "è¯„ä¼°å›½äº§æ›¿ä»£çš„è¿›å±•å’Œæœºä¼š"
            ],
            "ç«äº‰æ ¼å±€": [
                "ä½¿ç”¨CR5/CR10ç­‰é›†ä¸­åº¦æŒ‡æ ‡",
                "åˆ†æé¾™å¤´ä¼ä¸šçš„æ ¸å¿ƒç«äº‰åŠ›",
                "å…³æ³¨æ–°è¿›å…¥è€…å’Œæ½œåœ¨é¢ è¦†è€…",
                "è¯„ä¼°è¡Œä¸šå£å’çš„é«˜ä½"
            ],
            "æŠ•èµ„å»ºè®®": [
                "æŠ•èµ„å»ºè®®å¿…é¡»æœ‰æ˜ç¡®çš„é€»è¾‘æ”¯æ’‘",
                "åŒºåˆ†çŸ­æœŸæœºä¼šå’Œé•¿æœŸä»·å€¼",
                "æ˜ç¡®é£é™©æç¤ºå’Œåº”å¯¹ç­–ç•¥",
                "ç»™å‡ºå…·ä½“çš„æŠ•èµ„æ ‡çš„æˆ–æ–¹å‘"
            ]
        }
    
    def record_success(self, pattern: Dict):
        """è®°å½•æˆåŠŸçš„ç ”ç©¶æ¨¡å¼"""
        pattern["timestamp"] = datetime.datetime.now().isoformat()
        pattern["type"] = "success"
        self.successful_patterns.append(pattern)
        
        # é™åˆ¶å­˜å‚¨æ•°é‡
        if len(self.successful_patterns) > 100:
            self.successful_patterns = self.successful_patterns[-100:]
    
    def record_failure(self, pattern: Dict, reason: str):
        """è®°å½•å¤±è´¥æ¡ˆä¾‹"""
        pattern["timestamp"] = datetime.datetime.now().isoformat()
        pattern["type"] = "failure"
        pattern["reason"] = reason
        self.failed_patterns.append(pattern)
        
        if len(self.failed_patterns) > 50:
            self.failed_patterns = self.failed_patterns[-50:]
    
    def get_recommendations(self, industry: str, dimension: str) -> List[str]:
        """è·å–é’ˆå¯¹ç‰¹å®šç»´åº¦çš„å»ºè®®"""
        recommendations = self.best_practices.get(dimension, [])
        
        # ä»æˆåŠŸæ¨¡å¼ä¸­å­¦ä¹ 
        for pattern in self.successful_patterns[-10:]:
            if pattern.get("industry") == industry and pattern.get("dimension") == dimension:
                if pattern.get("key_insight"):
                    recommendations.append(f"[å†å²ç»éªŒ] {pattern['key_insight']}")
        
        return recommendations
    
    def update_quality_score(self, report_id: str, score: float):
        """æ›´æ–°ç ”æŠ¥è´¨é‡è¯„åˆ†"""
        self.quality_scores[report_id] = score


class MemoryManager:
    """
    å…¨ç»´æŠ•ç ”è®°å¿†ç³»ç»Ÿ - å¢å¼ºç‰ˆ
    æ”¯æŒï¼šPDFåŸæ–‡ã€Agentäº§å‡ºçš„äº‹å®ã€è§‚ç‚¹ã€ç»“è®ºã€æ­£æ–‡æ®µè½
    æ–°å¢ï¼šæ™ºèƒ½å­¦ä¹ ã€ç»éªŒç§¯ç´¯ã€çŸ¥è¯†å›¾è°±
    """

    def __init__(self, persist_dir: str):
        self.vector_store = ChromaVectorStore(persist_dir)
        self.retriever = VectorRetriever(self.vector_store)
        self.pdf_ingestor = PDFIngestor()
        
        # æ–‡æœ¬åˆ‡åˆ†å™¨
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=500, 
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", " ", ""]
        )
        
        # æ–°å¢ï¼šçŸ¥è¯†å›¾è°±å’Œç»éªŒç®¡ç†
        self.knowledge_graph = IndustryKnowledgeGraph()
        self.experience = ResearchExperience()
        
        # ç¼“å­˜
        self._cache: Dict[str, Any] = {}
        self._cache_ttl = 3600  # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_insights": 0,
            "total_recalls": 0,
            "industries_covered": set(),
            "last_update": None
        }

    # ------------------ å­˜å…¥ (Write) ------------------

    def save_insight(self, content: str, category: str, metadata: dict):
        """
        æ ¸å¿ƒæ–¹æ³•ï¼šå­˜å‚¨ Agent çš„äº§å‡º
        :param content: æ–‡æœ¬å†…å®¹
        :param category: 'fact' | 'opinion' | 'conclusion' | 'report_segment' | 'experience'
        :param metadata: {industry, year, province, focus, source_agent}
        """
        if not content:
            return

        # è‡ªåŠ¨è¡¥å…¨å…ƒæ•°æ®
        meta = metadata.copy()
        meta.update({
            "category": category,
            "ingest_time": datetime.datetime.now().isoformat(),
            "type": "agent_memory",
            "content_hash": hashlib.md5(content.encode()).hexdigest()[:8]
        })

        # æ ¹æ®å†…å®¹é•¿åº¦å†³å®šæ˜¯å¦åˆ‡åˆ†
        if len(content) < 500:
            chunks = [content]
        else:
            chunks = self.splitter.split_text(content)
            
        metadatas = [meta for _ in chunks]
        self.vector_store.add_texts(chunks, metadatas)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats["total_insights"] += len(chunks)
        if metadata.get("industry"):
            self.stats["industries_covered"].add(metadata["industry"])
        self.stats["last_update"] = datetime.datetime.now().isoformat()
        
        print(f"ğŸ§  [Memory] å·²å­˜å‚¨ {len(chunks)} æ¡ {category} è®°å¿†")

    def save_research_experience(self, industry: str, dimension: str, 
                                  insight: str, success: bool = True):
        """
        ä¿å­˜ç ”ç©¶ç»éªŒ
        :param industry: è¡Œä¸š
        :param dimension: ç ”ç©¶ç»´åº¦
        :param insight: å…³é”®æ´å¯Ÿ
        :param success: æ˜¯å¦æˆåŠŸ
        """
        pattern = {
            "industry": industry,
            "dimension": dimension,
            "key_insight": insight
        }
        
        if success:
            self.experience.record_success(pattern)
        else:
            self.experience.record_failure(pattern, "è´¨é‡ä¸è¾¾æ ‡")
        
        # åŒæ—¶å­˜å…¥å‘é‡åº“
        self.save_insight(
            content=f"[{industry}][{dimension}] {insight}",
            category="experience",
            metadata={
                "industry": industry,
                "dimension": dimension,
                "success": success
            }
        )

    def ingest_pdf(self, file_path: str, metadata: dict):
        """å¯¼å…¥PDFæ–‡æ¡£"""
        raw_text = self.pdf_ingestor.ingest(file_path)
        chunks = self.splitter.split_text(raw_text)
        
        # å¢å¼ºå…ƒæ•°æ®
        enhanced_meta = metadata.copy()
        enhanced_meta["source_type"] = "pdf"
        enhanced_meta["file_path"] = file_path
        
        metadatas = [enhanced_meta for _ in chunks]
        self.vector_store.add_texts(chunks, metadatas)
        
        print(f"ğŸ“„ [Memory] å·²å¯¼å…¥PDF: {file_path}, {len(chunks)} ä¸ªç‰‡æ®µ")

    # ------------------ å¬å› (Read) ------------------

    def recall_memory(self, query: str, category: str = None, 
                      k: int = 5, industry: str = None) -> List[Dict]:
        """
        ç²¾å‡†å¬å›ï¼šæ”¯æŒæŒ‰ category å’Œ industry è¿‡æ»¤
        """
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{query}_{category}_{k}_{industry}"
        if cache_key in self._cache:
            cached = self._cache[cache_key]
            if (datetime.datetime.now() - cached["time"]).seconds < self._cache_ttl:
                return cached["data"]
        
        # å¬å›æ›´å¤šç»“æœä»¥ä¾¿è¿‡æ»¤
        results = self.retriever.retrieve(query, k=k * 3)
        
        # è¿‡æ»¤
        filtered_results = []
        for doc in results:
            # å…¼å®¹ä¸åŒçš„è¿”å›æ ¼å¼
            if hasattr(doc, 'metadata'):
                meta = doc.metadata
                content = doc.page_content
            elif isinstance(doc, dict):
                meta = doc.get('metadata', {})
                content = doc.get('content', doc.get('page_content', ''))
            else:
                continue
            
            # æŒ‰categoryè¿‡æ»¤
            if category and meta.get('category') != category:
                continue
            
            # æŒ‰industryè¿‡æ»¤
            if industry and meta.get('industry') != industry:
                continue
            
            filtered_results.append({
                "content": content,
                "metadata": meta
            })
            
            if len(filtered_results) >= k:
                break
        
        # æ›´æ–°ç¼“å­˜
        self._cache[cache_key] = {
            "data": filtered_results,
            "time": datetime.datetime.now()
        }
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats["total_recalls"] += 1
        
        return filtered_results

    def recall_similar_reports(self, industry: str, province: str = None, 
                                k: int = 3) -> List[Dict]:
        """
        å¬å›ç›¸ä¼¼çš„å†å²ç ”æŠ¥
        ç”¨äºå­¦ä¹ æˆåŠŸçš„ç ”ç©¶æ¨¡å¼
        """
        query = f"{province or ''} {industry} è¡Œä¸šç ”ç©¶æŠ¥å‘Š"
        return self.recall_memory(
            query=query,
            category="report_segment",
            k=k,
            industry=industry
        )

    # ------------------ æ™ºèƒ½å»ºè®® ------------------

    def get_research_suggestions(self, industry: str, 
                                  dimension: str = None) -> Dict[str, Any]:
        """
        è·å–ç ”ç©¶å»ºè®®
        åŸºäºçŸ¥è¯†å›¾è°±å’Œå†å²ç»éªŒ
        """
        suggestions = {
            "related_industries": self.knowledge_graph.get_related_industries(industry),
            "supply_chain_template": self.knowledge_graph.get_supply_chain_template(industry),
            "key_metrics": self.knowledge_graph.get_key_metrics(industry),
            "best_practices": [],
            "historical_insights": []
        }
        
        # è·å–æœ€ä½³å®è·µ
        if dimension:
            suggestions["best_practices"] = self.experience.get_recommendations(
                industry, dimension
            )
        else:
            for dim in ["æ•°æ®å¼•ç”¨", "äº§ä¸šé“¾åˆ†æ", "ç«äº‰æ ¼å±€", "æŠ•èµ„å»ºè®®"]:
                suggestions["best_practices"].extend(
                    self.experience.get_recommendations(industry, dim)
                )
        
        # å¬å›å†å²æ´å¯Ÿ
        historical = self.recall_memory(
            query=f"{industry} æŠ•èµ„æœºä¼š é£é™©",
            category="conclusion",
            k=5,
            industry=industry
        )
        suggestions["historical_insights"] = [
            h["content"] for h in historical
        ]
        
        return suggestions

    def get_industry_context(self, industry: str, province: str = None) -> str:
        """
        è·å–è¡Œä¸šä¸Šä¸‹æ–‡ä¿¡æ¯
        ç”¨äºå¢å¼ºAgentçš„èƒŒæ™¯çŸ¥è¯†
        """
        context_parts = []
        
        # 1. ç›¸å…³è¡Œä¸š
        relations = self.knowledge_graph.get_related_industries(industry)
        if relations.get("upstream"):
            context_parts.append(f"ä¸Šæ¸¸å…³è”è¡Œä¸šï¼š{', '.join(relations['upstream'])}")
        if relations.get("downstream"):
            context_parts.append(f"ä¸‹æ¸¸å…³è”è¡Œä¸šï¼š{', '.join(relations['downstream'])}")
        
        # 2. å…³é”®æŒ‡æ ‡
        metrics = self.knowledge_graph.get_key_metrics(industry)
        if metrics:
            context_parts.append(f"å…³é”®ç ”ç©¶æŒ‡æ ‡ï¼š{', '.join(metrics)}")
        
        # 3. å†å²ç ”ç©¶ç»éªŒ
        experiences = self.recall_memory(
            query=f"{industry} ç ”ç©¶ç»éªŒ",
            category="experience",
            k=3
        )
        if experiences:
            context_parts.append("å†å²ç ”ç©¶ç»éªŒï¼š")
            for exp in experiences:
                context_parts.append(f"  - {exp['content'][:100]}...")
        
        return "\n".join(context_parts)

    # ------------------ ç»Ÿè®¡ä¸ç»´æŠ¤ ------------------

    def get_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_insights": self.stats["total_insights"],
            "total_recalls": self.stats["total_recalls"],
            "industries_covered": list(self.stats["industries_covered"]),
            "last_update": self.stats["last_update"],
            "successful_patterns": len(self.experience.successful_patterns),
            "failed_patterns": len(self.experience.failed_patterns)
        }

    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        self._cache.clear()
        print("ğŸ§¹ [Memory] ç¼“å­˜å·²æ¸…ç†")

    def export_knowledge(self, output_path: str):
        """å¯¼å‡ºçŸ¥è¯†åº“"""
        knowledge = {
            "industry_relations": self.knowledge_graph.industry_relations,
            "supply_chain_templates": self.knowledge_graph.supply_chain_templates,
            "key_metrics": self.knowledge_graph.key_metrics,
            "best_practices": self.experience.best_practices,
            "successful_patterns": self.experience.successful_patterns[-20:],
            "stats": self.get_stats()
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(knowledge, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“¤ [Memory] çŸ¥è¯†åº“å·²å¯¼å‡º: {output_path}")

    def import_knowledge(self, input_path: str):
        """å¯¼å…¥çŸ¥è¯†åº“"""
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                knowledge = json.load(f)
            
            # åˆå¹¶è¡Œä¸šå…³ç³»
            self.knowledge_graph.industry_relations.update(
                knowledge.get("industry_relations", {})
            )
            
            # åˆå¹¶äº§ä¸šé“¾æ¨¡æ¿
            self.knowledge_graph.supply_chain_templates.update(
                knowledge.get("supply_chain_templates", {})
            )
            
            # åˆå¹¶å…³é”®æŒ‡æ ‡
            self.knowledge_graph.key_metrics.update(
                knowledge.get("key_metrics", {})
            )
            
            # åˆå¹¶æœ€ä½³å®è·µ
            for key, practices in knowledge.get("best_practices", {}).items():
                if key in self.experience.best_practices:
                    self.experience.best_practices[key].extend(practices)
                else:
                    self.experience.best_practices[key] = practices
            
            print(f"ğŸ“¥ [Memory] çŸ¥è¯†åº“å·²å¯¼å…¥: {input_path}")
            
        except Exception as e:
            print(f"âš ï¸ [Memory] çŸ¥è¯†åº“å¯¼å…¥å¤±è´¥: {e}")


# å…¨å±€å•ä¾‹
memory_manager = MemoryManager(persist_dir="./knowledge_base/vector_store")
